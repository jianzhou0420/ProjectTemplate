# helper package
# try:
#     import warnings
#     warnings.filterwarnings("ignore", message="Gimbal lock detected. Setting third angle to zero")
#     warnings.filterwarnings("ignore", category=FutureWarning, message=".*torch.cuda.amp.custom_bwd.*")
#     warnings.filterwarnings("ignore", category=FutureWarning, message=".*torch.cuda.amp.custom_fwd.*")
# except:
#     pass


import pathlib
import os
import os
from typing import Type, Dict, Any
import copy
import random
import numpy as np
# framework package

import torch
from torch.utils.data import DataLoader, Dataset


from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger, WandbLogger
import pytorch_lightning as pl
# equidiff package
from projectname.policy.diffusion_unet_hybrid_image_policy import DiffusionUnetHybridImagePolicy
from projectname.policy.diffusion_transformer_hybrid_image_policy import DiffusionTransformerHybridImagePolicy
from projectname.dataset.base_dataset import BaseImageDataset
from projectname.env_runner.base_image_runner import BaseImageRunner

from projectname.common.pytorch_util import dict_apply, optimizer_to
from projectname.model.diffusion.ema_model import EMAModel
from projectname.model.common.lr_scheduler import get_scheduler
# Hydra specific imports
import hydra
from hydra.utils import instantiate
from omegaconf import DictConfig, OmegaConf
from projectname.config.config_hint import AppConfig

import pathlib
from omegaconf import OmegaConf
import hydra
import sys
import mimicgen
from natsort import natsorted

# use line-buffering for both stdout and stderr
sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)
sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ['TORCH_USE_CUDA_DSA'] = "1"
os.environ['HYDRA_FULL_ERROR'] = "1"

torch.set_float32_matmul_precision('medium')

# ---------------------------------------------------------------
# region 0. Tools


def load_pretrained_weights(model, ckpt_path):
    """
    Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÂπ∂Ê†πÊçÆÁ≠ñÁï•ÂÜªÁªìÂèÇÊï∞„ÄÇ

    Ê≠§ÂáΩÊï∞ÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºö
    1. ËÆ∞ÂΩïÊ®°Âûã‰∏≠ÂàùÂßãÁä∂ÊÄÅÂ∞±‰∏∫ÂÜªÁªìÁöÑÂèÇÊï∞„ÄÇ
    2. ‰ªéÊåáÂÆöÁöÑ `ckpt_path` Âä†ËΩΩÊùÉÈáç„ÄÇ
    3. Â∞ÜÊùÉÈáçÂä†ËΩΩÂà∞Ê®°Âûã‰∏≠ÂåπÈÖçÁöÑÂ±Ç„ÄÇ
    4. Âº∫Âà∂ÂÜªÁªìÊ®°Âûã‰∏≠ÊâÄÊúâÁöÑ 'clip' Â≠êÊ®°ÂùóÂèÇÊï∞ÔºåÂπ∂Â∞ÜÂÖ∂ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè„ÄÇ
    5. ÂÜªÁªìÊâÄÊúâÂÖ∂‰ªñ‰ªéÊ£ÄÊü•ÁÇπÊàêÂäüÂä†ËΩΩÁöÑÂèÇÊï∞„ÄÇ
    6. Á°Æ‰øùÂàùÂßãÁä∂ÊÄÅ‰∏∫ÂÜªÁªìÁöÑÂèÇÊï∞‰øùÊåÅÂÜªÁªì„ÄÇ
    7. ‰øùÊåÅÊ®°Âûã‰∏≠ÂÖ∂‰ΩôÂèÇÊï∞ÔºàÂ¶ÇÊñ∞ÁöÑÂàÜÁ±ªÂ§¥Ôºâ‰∏∫ÂèØËÆ≠ÁªÉÁä∂ÊÄÅ„ÄÇ

    Args:
        model (nn.Module): ÈúÄË¶ÅÂä†ËΩΩÊùÉÈáçÂíåËÆæÁΩÆÊ¢ØÂ∫¶ÁöÑÊ®°Âûã„ÄÇ
        ckpt_path (str): È¢ÑËÆ≠ÁªÉÊùÉÈáçÊñá‰ª∂Ôºà.pthÔºâÁöÑË∑ØÂæÑ„ÄÇ

    Returns:
        nn.Module: Â§ÑÁêÜÂÆåÊàêÂêéÁöÑÊ®°Âûã„ÄÇ
    """
    # --------------------------------------------------------------------------
    # ‚ú® Êñ∞Â¢ûÊ≠•È™§ÔºöËÆ∞ÂΩïÂàùÂßãÂÜªÁªìÁä∂ÊÄÅ
    # --------------------------------------------------------------------------
    initially_frozen_keys = {name for name, param in model.named_parameters() if not param.requires_grad}
    if initially_frozen_keys:
        print(f"Ê£ÄÊµãÂà∞ {len(initially_frozen_keys)} ‰∏™ÂèÇÊï∞Âú®ÂáΩÊï∞Ë∞ÉÁî®ÂâçÂ∑≤Ë¢´ËÆæÁΩÆ‰∏∫ÂÜªÁªìÁä∂ÊÄÅ„ÄÇËøô‰∫õÂèÇÊï∞Â∞Ü‰øùÊåÅÂÜªÁªì„ÄÇ")
        # for name in initially_frozen_keys:
        #     print(f"  - ÂàùÂßãÂÜªÁªì: {name}")

    if not ckpt_path:
        print("Êú™Êèê‰æõÊùÉÈáçË∑ØÂæÑÔºåË∑≥ËøáÊùÉÈáçÂä†ËΩΩËøáÁ®ã„ÄÇ")
        # Âç≥‰Ωø‰∏çÂä†ËΩΩÊùÉÈáçÔºåÊàë‰ª¨‰ªçÁÑ∂ÈúÄË¶ÅÂÜªÁªìCLIPÂíå‰øùÊåÅÂàùÂßãÂÜªÁªìÁä∂ÊÄÅ
        if hasattr(model, 'clip'):
            print("Ê≠£Âú®ÂÜªÁªì CLIP Ê®°ÂùóÂπ∂ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè...")
            model.clip.eval()
            for name, param in model.clip.named_parameters():
                param.requires_grad = False
                print(f"üßä [Âº∫Âà∂ÂÜªÁªì] {name}")
        return model

    # --------------------------------------------------------------------------
    # Á¨¨1Ê≠•ÔºöËØÜÂà´Âá∫ÂèØ‰ª•ÂÆâÂÖ®Âä†ËΩΩÁöÑÊùÉÈáç (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    print(f"Ê≠£Âú®‰ªé '{ckpt_path}' Âä†ËΩΩÊùÉÈáç...")
    pretrained_dict = torch.load(ckpt_path, map_location='cpu')['state_dict']

    new_model_dict = model.state_dict()
    loadable_keys = set()
    filtered_dict = {}

    print("Ê≠£Âú®Á≠õÈÄâÂÖºÂÆπÁöÑÊùÉÈáç...")
    for k, v in pretrained_dict.items():
        if k in new_model_dict and new_model_dict[k].shape == v.shape:
            filtered_dict[k] = v
            loadable_keys.add(k)
    print(f"ËØÜÂà´Âá∫ {len(loadable_keys)} ‰∏™ÂèÇÊï∞ÂèØ‰ª•‰ªé checkpoint ÂÆâÂÖ®Âä†ËΩΩ„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨2Ê≠•ÔºöÂä†ËΩΩÁ≠õÈÄâÂêéÁöÑÊùÉÈáç (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    new_model_dict.update(filtered_dict)
    model.load_state_dict(new_model_dict)
    print("Â∑≤ÊàêÂäüÂä†ËΩΩÊâÄÊúâÂÖºÂÆπÁöÑÊùÉÈáç„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨3Ê≠•ÔºöÂº∫Âà∂ËÆæÁΩÆ CLIP Ê®°Âùó‰∏∫ËØÑ‰º∞Ê®°Âºè (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    if hasattr(model, 'clip'):
        print("Ê≠£Âú®Â∞Ü CLIP Ê®°ÂùóËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè (model.clip.eval())...")
        model.clip.eval()
    else:
        print("‚ö†Ô∏è  Ë≠¶Âëä: Ê®°Âûã‰∏≠Êú™ÊâæÂà∞Âêç‰∏∫ 'clip' ÁöÑÂ±ûÊÄßÔºåÊó†Ê≥ïËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨4Ê≠•ÔºàÂ∑≤‰øÆÊîπÔºâÔºöÊ†πÊçÆÂä†ËΩΩÊÉÖÂÜµ„ÄÅÊ®°ÂùóÂêçÁß∞ÂíåÂàùÂßãÁä∂ÊÄÅÊô∫ËÉΩÂú∞ËÆæÁΩÆÊ¢ØÂ∫¶
    # --------------------------------------------------------------------------
    print("Ê≠£Âú®Êô∫ËÉΩÂú∞ËÆæÁΩÆÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ËÆ°ÁÆóÁä∂ÊÄÅ...")
    trainable_params = 0
    frozen_params = 0

    for name, param in model.named_parameters():
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶Âú®ÂàùÂßãÊó∂Â∞±Â∑≤ÂÜªÁªì
        is_initially_frozen = name in initially_frozen_keys
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶Â±û‰∫éCLIPÊ®°Âùó
        is_clip_param = name.startswith('clip.')
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶‰ªécheckpointÂä†ËΩΩ
        is_loaded_from_ckpt = name in loadable_keys

        # Êô∫ËÉΩÈÄªËæëÔºöÊ£ÄÊü•ÂÅèÁΩÆÂØπÂ∫îÁöÑÊùÉÈáçÊòØÂê¶‰πüË¢´Âä†ËΩΩ‰∫Ü
        if name.endswith('.bias') and not is_clip_param and not is_initially_frozen:
            weight_name = name.replace('.bias', '.weight')
            if weight_name not in loadable_keys:
                is_loaded_from_ckpt = False
                print(f"‚ÑπÔ∏è  Ê≥®ÊÑè: ÂÅèÁΩÆ '{name}' Â∞Ü‰øùÊåÅÂèØËÆ≠ÁªÉÔºåÂõ†‰∏∫ÂÖ∂ÂØπÂ∫îÁöÑÊùÉÈáç '{weight_name}' Êú™Ë¢´Âä†ËΩΩ„ÄÇ")

        # ÊúÄÁªàÂÜªÁªìÂÜ≥Á≠ñÔºöÂè™Ë¶ÅÊòØÂàùÂßãÂÜªÁªì„ÄÅCLIPÂèÇÊï∞Êàñ‰ªécheckpointÂä†ËΩΩÁöÑÂèÇÊï∞ÔºåÂ∞±ÂÜªÁªì
        if is_initially_frozen or is_clip_param or is_loaded_from_ckpt:
            param.requires_grad = False
            frozen_params += 1
        else:
            param.requires_grad = True
            trainable_params += 1

    print(f"Á≠ñÁï•ÊâßË°åÂÆåÊØïÔºö{frozen_params} ‰∏™ÂèÇÊï∞Ë¢´ÂÜªÁªìÔºå{trainable_params} ‰∏™ÂèÇÊï∞‰øùÊåÅÂèØËÆ≠ÁªÉ„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨5Ê≠•ÔºöÊúÄÁªàÈ™åËØÅ (Â∑≤‰øÆÊîπ)
    # --------------------------------------------------------------------------
    print("\n--- ÊúÄÁªàÊ®°ÂûãÊ¢ØÂ∫¶Áä∂ÊÄÅÈ™åËØÅ ---")
    for name, param in model.named_parameters():
        status = "üßä [Â∑≤ÂÜªÁªì]" if not param.requires_grad else "‚úÖ [ÂèØËÆ≠ÁªÉ]"
        reason = ""
        if not param.requires_grad:
            if name in initially_frozen_keys:
                reason = "(ÂéüÂõ†: ÂàùÂßãÁä∂ÊÄÅ‰∏∫ÂÜªÁªì)"
            elif name.startswith('clip.'):
                reason = "(ÂéüÂõ†: CLIPÊ®°Âùó)"
            elif name in loadable_keys:
                reason = "(ÂéüÂõ†: ‰ªéckptÂä†ËΩΩ)"
        print(f"{status} {name} {reason}")
    print("---------------------------------")

    return model


def load_pretrained_weights_DP_T(model, ckpt_path):

    # --------------------------------------------------------------------------
    # ‚ú® Êñ∞Â¢ûÊ≠•È™§ÔºöËÆ∞ÂΩïÂàùÂßãÂÜªÁªìÁä∂ÊÄÅ
    # --------------------------------------------------------------------------
    initially_frozen_keys = {name for name, param in model.named_parameters() if not param.requires_grad}
    if initially_frozen_keys:
        print(f"Ê£ÄÊµãÂà∞ {len(initially_frozen_keys)} ‰∏™ÂèÇÊï∞Âú®ÂáΩÊï∞Ë∞ÉÁî®ÂâçÂ∑≤Ë¢´ËÆæÁΩÆ‰∏∫ÂÜªÁªìÁä∂ÊÄÅ„ÄÇËøô‰∫õÂèÇÊï∞Â∞Ü‰øùÊåÅÂÜªÁªì„ÄÇ")
        # for name in initially_frozen_keys:
        #     print(f"  - ÂàùÂßãÂÜªÁªì: {name}")

    if not ckpt_path:
        print("Êú™Êèê‰æõÊùÉÈáçË∑ØÂæÑÔºåË∑≥ËøáÊùÉÈáçÂä†ËΩΩËøáÁ®ã„ÄÇ")
        # Âç≥‰Ωø‰∏çÂä†ËΩΩÊùÉÈáçÔºåÊàë‰ª¨‰ªçÁÑ∂ÈúÄË¶ÅÂÜªÁªìCLIPÂíå‰øùÊåÅÂàùÂßãÂÜªÁªìÁä∂ÊÄÅ
        if hasattr(model, 'clip'):
            print("Ê≠£Âú®ÂÜªÁªì CLIP Ê®°ÂùóÂπ∂ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè...")
            model.clip.eval()
            for name, param in model.clip.named_parameters():
                param.requires_grad = False
                print(f"üßä [Âº∫Âà∂ÂÜªÁªì] {name}")
        return model

    manully_unfrozen_keys = [
        "model.pos_emb",
        # "model.input_emb.weight",
        # "model.input_emb.bias",
        "model.encoder.0.weight",
        "model.encoder.0.bias",
        "model.encoder.2.weight",
        "model.encoder.2.bias",
    ]
    # --------------------------------------------------------------------------
    # Á¨¨1Ê≠•ÔºöËØÜÂà´Âá∫ÂèØ‰ª•ÂÆâÂÖ®Âä†ËΩΩÁöÑÊùÉÈáç (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    print(f"Ê≠£Âú®‰ªé '{ckpt_path}' Âä†ËΩΩÊùÉÈáç...")
    pretrained_dict = torch.load(ckpt_path, map_location='cpu')['state_dict']

    new_model_dict = model.state_dict()
    loadable_keys = set()
    filtered_dict = {}

    print("Ê≠£Âú®Á≠õÈÄâÂÖºÂÆπÁöÑÊùÉÈáç...")
    for k, v in pretrained_dict.items():
        if k in new_model_dict and new_model_dict[k].shape == v.shape:
            filtered_dict[k] = v
            loadable_keys.add(k)
    print(f"ËØÜÂà´Âá∫ {len(loadable_keys)} ‰∏™ÂèÇÊï∞ÂèØ‰ª•‰ªé checkpoint ÂÆâÂÖ®Âä†ËΩΩ„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨2Ê≠•ÔºöÂä†ËΩΩÁ≠õÈÄâÂêéÁöÑÊùÉÈáç (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    new_model_dict.update(filtered_dict)
    model.load_state_dict(new_model_dict)
    print("Â∑≤ÊàêÂäüÂä†ËΩΩÊâÄÊúâÂÖºÂÆπÁöÑÊùÉÈáç„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨3Ê≠•ÔºöÂº∫Âà∂ËÆæÁΩÆ CLIP Ê®°Âùó‰∏∫ËØÑ‰º∞Ê®°Âºè (ÈÄªËæë‰∏çÂèò)
    # --------------------------------------------------------------------------
    if hasattr(model, 'clip'):
        print("Ê≠£Âú®Â∞Ü CLIP Ê®°ÂùóËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè (model.clip.eval())...")
        model.clip.eval()
    else:
        print("‚ö†Ô∏è  Ë≠¶Âëä: Ê®°Âûã‰∏≠Êú™ÊâæÂà∞Âêç‰∏∫ 'clip' ÁöÑÂ±ûÊÄßÔºåÊó†Ê≥ïËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè„ÄÇ")

    # --------------------------------------------------------------------------
    # Á¨¨4Ê≠•ÔºàÂ∑≤‰øÆÊîπÔºâÔºöÊ†πÊçÆÂä†ËΩΩÊÉÖÂÜµ„ÄÅÊ®°ÂùóÂêçÁß∞ÂíåÂàùÂßãÁä∂ÊÄÅÊô∫ËÉΩÂú∞ËÆæÁΩÆÊ¢ØÂ∫¶
    # --------------------------------------------------------------------------
    print("Ê≠£Âú®Êô∫ËÉΩÂú∞ËÆæÁΩÆÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ËÆ°ÁÆóÁä∂ÊÄÅ...")
    trainable_params = 0
    frozen_params = 0

    for name, param in model.named_parameters():
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶Âú®ÂàùÂßãÊó∂Â∞±Â∑≤ÂÜªÁªì
        is_initially_frozen = name in initially_frozen_keys
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶Â±û‰∫éCLIPÊ®°Âùó
        is_clip_param = name.startswith('clip.')
        # Ê£ÄÊü•ÂèÇÊï∞ÊòØÂê¶‰ªécheckpointÂä†ËΩΩ
        is_loaded_from_ckpt = name in loadable_keys

        is_manully_unfrozen = name in manully_unfrozen_keys

        # Êô∫ËÉΩÈÄªËæëÔºöÊ£ÄÊü•ÂÅèÁΩÆÂØπÂ∫îÁöÑÊùÉÈáçÊòØÂê¶‰πüË¢´Âä†ËΩΩ‰∫Ü
        if name.endswith('.bias') and not is_clip_param and not is_initially_frozen:
            weight_name = name.replace('.bias', '.weight')
            if weight_name not in loadable_keys:
                is_loaded_from_ckpt = False
                print(f"‚ÑπÔ∏è  Ê≥®ÊÑè: ÂÅèÁΩÆ '{name}' Â∞Ü‰øùÊåÅÂèØËÆ≠ÁªÉÔºåÂõ†‰∏∫ÂÖ∂ÂØπÂ∫îÁöÑÊùÉÈáç '{weight_name}' Êú™Ë¢´Âä†ËΩΩ„ÄÇ")

        # ÊúÄÁªàÂÜªÁªìÂÜ≥Á≠ñÔºöÂè™Ë¶ÅÊòØÂàùÂßãÂÜªÁªì„ÄÅCLIPÂèÇÊï∞Êàñ‰ªécheckpointÂä†ËΩΩÁöÑÂèÇÊï∞ÔºåÂ∞±ÂÜªÁªì
        if (is_initially_frozen or is_clip_param or is_loaded_from_ckpt) and not is_manully_unfrozen:
            param.requires_grad = False
            frozen_params += 1
        else:
            param.requires_grad = True
            trainable_params += 1

    # --------------------------------------------------------------------------
    # Á¨¨6Ê≠•Ôºö ÂèØËÄªÁöÑÁöÑÊâãÂä®‰øÆÊ≠£
    # --------------------------------------------------------------------------

    print(f"Á≠ñÁï•ÊâßË°åÂÆåÊØïÔºö{frozen_params} ‰∏™ÂèÇÊï∞Ë¢´ÂÜªÁªìÔºå{trainable_params} ‰∏™ÂèÇÊï∞‰øùÊåÅÂèØËÆ≠ÁªÉ„ÄÇ")
    # --------------------------------------------------------------------------
    # Á¨¨7Ê≠•ÔºöÊúÄÁªàÈ™åËØÅ
    # --------------------------------------------------------------------------
    print("\n--- ÊúÄÁªàÊ®°ÂûãÊ¢ØÂ∫¶Áä∂ÊÄÅÈ™åËØÅ ---")
    for name, param in model.named_parameters():
        status = "üßä [Â∑≤ÂÜªÁªì]" if not param.requires_grad else "‚úÖ [ÂèØËÆ≠ÁªÉ]"
        reason = ""
        if not param.requires_grad:
            if name in initially_frozen_keys:
                reason = "(ÂéüÂõ†: ÂàùÂßãÁä∂ÊÄÅ‰∏∫ÂÜªÁªì)"
            elif name.startswith('clip.'):
                reason = "(ÂéüÂõ†: CLIPÊ®°Âùó)"
            elif name in loadable_keys:
                reason = "(ÂéüÂõ†: ‰ªéckptÂä†ËΩΩ)"
        print(f"{status} {name} {reason}")
    print("---------------------------------")

    return model


def set_all_seeds(seed: int = 42) -> None:
    """
    Sets the random seed for reproducibility across all key libraries.

    Args:
        seed (int): The integer value to use as the random seed.
    """
    # 1. Set seed for Python's built-in random module
    random.seed(seed)

    # 2. Set seed for NumPy
    np.random.seed(seed)

    # 3. Set seed for PyTorch on CPU
    torch.manual_seed(seed)

    # 4. Set seeds for PyTorch on GPU(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups

    # 5. Set a fixed value for the hash seed
    os.environ['PYTHONHASHSEED'] = str(seed)

    # 6. Make PyTorch operations deterministic
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"Global random seed set to {seed} for reproducibility.")


def seed_worker(worker_id: int) -> None:
    """
    Sets the random seed for each worker process to ensure reproducibility
    of data loading and augmentation.
    """
    # The worker seed is derived from the main process's random state
    worker_seed = torch.initial_seed() % 2**32
    random.seed(worker_seed)
    np.random.seed(worker_seed)
    print(f"Worker {worker_id} has been seeded with {worker_seed}")

# ---------------------------------------------------------------
# region 1. Trainer


class Trainer_all(pl.LightningModule):
    def __init__(self, cfg):
        super().__init__()
        self.save_hyperparameters()
        self.cfg = cfg
        task_type = cfg.train_mode
        using_transformers = cfg.get("using_transformers", False)

        if task_type == 'stage2' or task_type == 'stage2_rollout':
            ckpt_path = cfg.ckpt_path
            policy: DiffusionUnetHybridImagePolicy = hydra.utils.instantiate(cfg.policy)
            policy = load_pretrained_weights(policy, ckpt_path) if not using_transformers else load_pretrained_weights_DP_T(policy, ckpt_path)
        elif task_type == 'stage1' or task_type == 'stage1_pure':
            policy: DiffusionUnetHybridImagePolicy = hydra.utils.instantiate(cfg.policy)
        elif task_type == 'normal' or task_type == 'normal_rollout':
            policy: DiffusionUnetHybridImagePolicy = hydra.utils.instantiate(cfg.policy)
        else:
            raise ValueError(f"Unsupported task type: {task_type}, check config.train_mode")

        if cfg.training.get("use_ema", False):
            policy_ema: DiffusionUnetHybridImagePolicy = copy.deepcopy(policy)
            ema_handler: EMAModel = hydra.utils.instantiate(
                cfg.ema,
                model=policy_ema,)
            self.ema_handler = ema_handler
            self.policy_ema = policy_ema.to(self.device)

        self.policy = policy
        self.train_sampling_batch = None

        # debug
        # for name, param in policy.named_parameters():
        #     print(name)

    def setup(self, stage='fit'):
        if stage == 'fit':
            self.normalizer = self.trainer.datamodule.normalizer
            self.policy.set_normalizer(self.normalizer)
            if self.cfg.training.get("use_ema", False):
                self.policy_ema.set_normalizer(self.normalizer)

        return

    def training_step(self, batch):
        # model = self.policy
        # print("\n--- ÊúÄÁªàÊ®°ÂûãÊ¢ØÂ∫¶Áä∂ÊÄÅÈ™åËØÅ ---")
        # for name, param in model.named_parameters():
        #     if param.requires_grad:
        #         print(f"‚úÖ [ÂèØËÆ≠ÁªÉ] {name}")
        #     else:
        #         print(f"üßä [Â∑≤ÂÜªÁªì] {name}")
        # print("---------------------------------")

        if self.train_sampling_batch is None:
            self.train_sampling_batch = batch

        loss = self.policy.compute_loss(batch)
        self.logger.experiment.log({
            'train/train_loss': loss.item(),
            'train/lr': self.optimizers().param_groups[0]['lr'],
            'trainer/global_step': self.global_step,
            'trainer/epoch': self.current_epoch,
        }, step=self.global_step)
        # print('print for logging in Kubectl')
        return loss

    def on_train_batch_end(self, outputs, batch, batch_idx):
        """
        This hook is called after the training step and optimizer update.
        It's the perfect place to update the EMA weights.
        """
        if self.cfg.training.get("use_ema", False):
            self.ema_handler.step(self.policy)

    def validation_step(self, batch):

        loss = self.policy_ema.compute_loss(batch) if self.cfg.training.get("use_ema", False) else self.policy.compute_loss(batch)
        self.logger.experiment.log({
            'train/val_loss': loss.item(),
        }, step=self.global_step)
        return loss

    def configure_optimizers(self):
        cfg: DictConfig = self.cfg

        using_transformers = cfg.get("using_transformers", False)
        using_ACT = cfg.get("using_ACT", False)
        if using_transformers:
            # 1. cfg
            transformer_weight_decay = cfg.optimizer.transformer_weight_decay
            obs_encoder_weight_decay = cfg.optimizer.obs_encoder_weight_decay
            learning_rate = cfg.optimizer.learning_rate
            betas = cfg.optimizer.betas
            self.policy: DiffusionTransformerHybridImagePolicy

            # 2. optimizer, the same as the diffusion_transformer_hybrid_image_policy.py
            optim_groups = self.policy.model.get_optim_groups(
                weight_decay=transformer_weight_decay)
            optim_groups.append({
                "params": self.policy.obs_encoder.parameters(),
                "weight_decay": obs_encoder_weight_decay
            })
            optimizer = torch.optim.AdamW(
                optim_groups, lr=learning_rate, betas=betas
            )
            # end of 2. optimizer
            num_training_steps = self.trainer.estimated_stepping_batches

            # 3.lr_scheduler
            lr_scheduler = get_scheduler(
                cfg.training.lr_scheduler,
                optimizer=optimizer,
                num_warmup_steps=cfg.training.lr_warmup_steps,
                num_training_steps=int((
                    num_training_steps)
                    // cfg.training.gradient_accumulate_every),
                # pytorch assumes stepping LRScheduler every epoch
                # however huggingface diffusers steps it every batch
                last_epoch=self.global_step - 1
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": lr_scheduler,
                    "interval": "step",  # Make sure to step the scheduler every batch/step
                    "frequency": 1,
                },
            }

        if using_ACT:
            optimizer = self.policy.optimizer
            return optimizer

        num_training_steps = self.trainer.estimated_stepping_batches

        optimizer = hydra.utils.instantiate(cfg.optimizer, params=self.policy.parameters())
        lr_scheduler = get_scheduler(
            cfg.training.lr_scheduler,
            optimizer=optimizer,
            num_warmup_steps=cfg.training.lr_warmup_steps,
            num_training_steps=int((
                num_training_steps)
                // cfg.training.gradient_accumulate_every),
            # pytorch assumes stepping LRScheduler every epoch
            # however huggingface diffusers steps it every batch
            last_epoch=self.global_step - 1
        )
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": lr_scheduler,
                "interval": "step",  # Make sure to step the scheduler every batch/step
                "frequency": 1,
            },
        }

    def on_save_checkpoint(self, checkpoint: Dict[str, Any]):
        """
        This hook is called when a checkpoint is saved.
        We replace the full state_dict with ONLY the state_dict of the policy
        we want to save (either the training policy or the EMA one).
        """
        if self.cfg.training.use_ema:
            # Get the state_dict from your EMA model
            policy_state_to_save = self.policy_ema.state_dict()
        else:
            # Get the state_dict from the standard training model
            policy_state_to_save = self.policy.state_dict()

        # Overwrite the complete state_dict with only the policy's state
        checkpoint['state_dict'] = policy_state_to_save
# endregion
# ---------------------------------------------------------------


# ---------------------------------------------------------------
# region 2. DataModule
class MyDataModule(pl.LightningDataModule):
    def __init__(self, cfg: Any):
        super().__init__()
        self.cfg = cfg
        # We'll store the generator to ensure deterministic shuffling
        self.train_generator = None
        self.val_generator = None

    def setup(self, stage: str):
        if stage == 'fit':
            # Create the PyTorch Generator objects here based on the seed
            seed = self.cfg.training.seed
            self.train_generator = torch.Generator().manual_seed(seed)
            self.val_generator = torch.Generator().manual_seed(seed)

            # NOTE: Assuming these instantiate the datasets correctly
            dataset = hydra.utils.instantiate(self.cfg.task.dataset)
            val_dataset = dataset.get_validation_dataset()

            assert isinstance(dataset, BaseImageDataset)
            normalizer = dataset.get_normalizer()

            self.normalizer = normalizer
            self.dataset = dataset
            self.val_dataset = val_dataset

    def train_dataloader(self):
        # Pass the worker_init_fn and the generator to the DataLoader
        train_dataloader = DataLoader(
            self.dataset,
            **self.cfg.dataloader,
            worker_init_fn=seed_worker,
            generator=self.train_generator
        )
        return train_dataloader

    def val_dataloader(self):
        # Pass the worker_init_fn and the generator to the DataLoader
        val_dataloader = DataLoader(
            self.val_dataset,
            **self.cfg.val_dataloader,
            worker_init_fn=seed_worker,
            generator=self.val_generator
        )
        return val_dataloader
# endregion
# ---------------------------------------------------------------

# ---------------------------------------------------------------
# region 3. Callback


class RolloutCallback(pl.Callback):
    """
    A Callback to run a policy rollout in an environment periodically.
    """

    def __init__(self, env_runner_cfg: DictConfig, rollout_every_n_epochs: int = 1):
        super().__init__()
        env_runner: BaseImageRunner
        env_runner = hydra.utils.instantiate(
            env_runner_cfg,
            output_dir='data/outputs'
        )  # TODO:fix it

        assert isinstance(env_runner, BaseImageRunner)
        self.rollout_every_n_epochs = rollout_every_n_epochs
        self.env_runner = env_runner

    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: Trainer_all):
        """
        This hook is called after every validation epoch.
        """

        # Ensure we only run this every N epochs
        if (trainer.current_epoch + 1) % self.rollout_every_n_epochs != 0:
            return
        if pl_module.global_step <= 0:
            return
        runner_log = self.env_runner.run(pl_module.policy_ema)
        trainer.logger.experiment.log(runner_log, step=trainer.global_step)
        # cprint(f"Rollout completed at epoch {trainer.current_epoch}, step {trainer.global_step}.", "green", attrs=['bold'])
        # cprint(f"Rollout log: {runner_log}", "blue", attrs=['bold'])


class ActionMseLossForDiffusion(pl.Callback):
    """
    A Callback to compute the MSE loss of actions in the diffusion model.
    This is useful for training the diffusion model with action data.
    """

    def __init__(self, cfg: AppConfig):
        super().__init__()
        self.cfg = cfg

    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: Trainer_all):
        """
        This hook is called after every validation epoch.
        """
        if pl_module.global_step <= 0:
            return
        train_sampling_batch = pl_module.train_sampling_batch

        batch = dict_apply(train_sampling_batch, lambda x: x.to(pl_module.device, non_blocking=True))
        obs_dict = batch['obs']
        gt_action = batch['action']
        result = pl_module.policy_ema.predict_action(obs_dict)
        pred_action = result['action_pred']
        mse = torch.nn.functional.mse_loss(pred_action, gt_action)
        trainer.logger.experiment.log({
            'train/action_mse_loss': mse,
        }, step=trainer.global_step)


# endregion
# ---------------------------------------------------------------

# ---------------------------------------------------------------
# region Main


def train(cfg: AppConfig):
    set_all_seeds(cfg.training.seed)

    using_transformers = cfg.get("using_transformers", False)
    using_ACT = cfg.get("using_ACT", False)
    # 0. extra config processing

    cfg_env_runner = []
    dataset_path = []
    for key, value in cfg.train_tasks_meta.items():
        this_dataset_path = f"data/robomimic/datasets/{key}/{key}_abs_{cfg.dataset_tail}.hdf5"
        this_env_runner_cfg = copy.deepcopy(cfg.task.env_runner)
        this_env_runner_cfg.dataset_path = this_dataset_path
        this_env_runner_cfg.max_steps = value

        OmegaConf.resolve(this_env_runner_cfg)
        dataset_path.append(this_dataset_path)
        cfg_env_runner.append(this_env_runner_cfg)
    if using_ACT:
        cfg.policy.max_timesteps = this_env_runner_cfg.max_steps
    cfg.task.dataset.dataset_path = OmegaConf.create(dataset_path)

    OmegaConf.save(cfg, os.path.join(cfg.run_dir, 'config.yaml'))
    # 1. Define a unique name and directory for this specific run

    this_run_dir = cfg.run_dir
    run_name = cfg.run_name

    os.makedirs(os.path.join(this_run_dir, 'wandb'), exist_ok=True)  # Ensure the output directory exists

    ckpt_path = os.path.join(this_run_dir, 'checkpoints')
    # 2. Configure ModelCheckpoint to save in that specific directory

    checkpoint_callback = ModelCheckpoint(
        dirpath=ckpt_path,
        filename=f'{run_name}' + '_{epoch:03d}',
        every_n_epochs=cfg.training.checkpoint_every,
        save_top_k=-1,
        save_last=False,
        save_weights_only=True,
        save_on_train_epoch_end=True
    )

    # 3. Configure WandbLogger to use the same directory
    wandb_logger = WandbLogger(
        save_dir=this_run_dir,  # <-- Use save_dir to point to the same path
        config=OmegaConf.to_container(cfg, resolve=True),
        **cfg.logging,
    )

    #

    if cfg.train_mode == 'stage1':
        callback_list = [checkpoint_callback,
                         ActionMseLossForDiffusion(cfg),
                         ]
    elif cfg.train_mode == 'stage1_pure':
        callback_list = [checkpoint_callback,
                         ]
    elif cfg.train_mode == 'stage2':
        callback_list = [checkpoint_callback,
                         ActionMseLossForDiffusion(cfg),
                         ]

    elif cfg.train_mode == 'normal':
        callback_list = [checkpoint_callback,
                         ActionMseLossForDiffusion(cfg),
                         ]
    elif cfg.train_mode == 'stage2_rollout':
        rollout_callback_list = [RolloutCallback(cfg_env_runner[i], rollout_every_n_epochs=cfg.training.rollout_every) for i in range(len(cfg_env_runner))]
        callback_list = [checkpoint_callback,
                         ActionMseLossForDiffusion(cfg),
                         ]
        callback_list.extend(rollout_callback_list)
    elif cfg.train_mode == 'normal_rollout':
        rollout_callback_list = [RolloutCallback(cfg_env_runner[i], rollout_every_n_epochs=cfg.training.rollout_every) for i in range(len(cfg_env_runner))]
        callback_list = [checkpoint_callback,
                         ActionMseLossForDiffusion(cfg),
                         ]
        callback_list.extend(rollout_callback_list)
    else:

        raise ValueError(f"Unsupported task type: {cfg.train_mode}, check config.name")

    trainer = pl.Trainer(callbacks=callback_list,
                         max_epochs=int(cfg.training.num_epochs),
                         devices=[0],
                         strategy='auto',
                         logger=[wandb_logger],
                         use_distributed_sampler=False,
                         check_val_every_n_epoch=cfg.training.val_every,
                         )
    trainer_model = Trainer_all(cfg)
    data_module = MyDataModule(cfg)
    trainer.fit(trainer_model, datamodule=data_module)

    # if cfg.train_mode == 'stage2':
    #     scp_to_another_computer(
    #         local_path=this_run_dir,
    #         remote_path=os.path.join('/media/jian/ssd4t/tmp', run_name),
    #         hostname='10.12.65.19',
    #         username='jian',
    #     )
    #     wandb.finish()
    #     evaluate_run(
    #         seed=42,
    #         run_dir=this_run_dir,)


@hydra.main(
    version_base=None,
    config_path=str(pathlib.Path(__file__).parent.joinpath(
        'equi_diffpo', 'config'))
)
def main(cfg: OmegaConf):
    # resolve immediately so all the ${now:} resolvers
    # will use the same time.

    OmegaConf.resolve(cfg)
    train(cfg)


# endregion
# ---------------------------------------------------------------
if __name__ == '__main__':
    tasks_meta = {
        "A": {"name": "stack_d1", "average_steps": 108, },
        "B": {"name": "square_d2", "average_steps": 153, },
        "C": {"name": "coffee_d2", "average_steps": 224, },
        "D": {"name": "threading_d2", "average_steps": 227, },
        "E": {"name": "stack_three_d1", "average_steps": 255, },
        "F": {"name": "hammer_cleanup_d1", "average_steps": 286, },
        "G": {"name": "three_piece_assembly_d2", "average_steps": 335, },
        "H": {"name": "mug_cleanup_d1", "average_steps": 338, },
        "I": {"name": "nut_assembly_d0", "average_steps": 358, },
        "J": {"name": "kitchen_d1", "average_steps": 619, },
        "K": {"name": "pick_place_d0", "average_steps": 677, },
        "L": {"name": "coffee_preparation_d1", "average_steps": 687, },
    }

    max_steps = {meta['name']: int(meta['average_steps'] * 2.5) for task, meta in tasks_meta.items()}
    print(f"max_steps: {max_steps}")

    def get_ws_x_center(task_name):
        if task_name.startswith('kitchen_') or task_name.startswith('hammer_cleanup_'):
            return -0.2
        else:
            return 0.

    def get_ws_y_center(task_name):
        return 0.

    def get_train_tasks_meta(task_alphabet):
        task_alphabet_list = natsorted(task_alphabet)
        train_tasks_meta = dict()
        for task_alphabet in task_alphabet_list:
            task_name = tasks_meta[task_alphabet]['name']
            task_max_steps = max_steps[task_name]
            train_tasks_meta.update({task_name: task_max_steps})
        train_tasks_meta = OmegaConf.create(train_tasks_meta)
        return train_tasks_meta

    OmegaConf.register_new_resolver("get_train_tasks_meta", get_train_tasks_meta, replace=True)
    OmegaConf.register_new_resolver("get_ws_x_center", get_ws_x_center, replace=True)
    OmegaConf.register_new_resolver("get_ws_y_center", get_ws_y_center, replace=True)

    # allows arbitrary python code execution in configs using the ${eval:''} resolver
    OmegaConf.register_new_resolver("eval", eval, replace=True)

    main()
